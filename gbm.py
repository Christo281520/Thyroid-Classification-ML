# -*- coding: utf-8 -*-
"""GBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RtvMysuSKP0XTZNCUjRRknyzlvIYXaEa
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import mutual_info_classif

thyroid_data = pd.read_csv('https://raw.githubusercontent.com/Christo2810/ds-and-ml/refs/heads/main/balanced_thyroid_dataset%20(1).csv')
thyroid_data.head()

print(thyroid_data.columns)

print(thyroid_data['Tumor'])

thyroid_data.info()
print(thyroid_data.describe())

# Step 2: Data Preprocessing
# Check for missing values
print(thyroid_data.isnull().sum())

x = thyroid_data.drop(columns=['Category'], axis=1)
y = thyroid_data['Category']

# Normalize 'TSH Measured' and 'Tumor' features
normalize_attributes = ['TSH Measured', 'Tumor']
x[normalize_attributes] = StandardScaler().fit_transform(x[normalize_attributes])

# Step 2: Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Step 3: Feature selection using mutual information
mutual_info = mutual_info_classif(x_train, y_train)
feature_scores = pd.DataFrame({'Feature': x_train.columns, 'Mutual Info': mutual_info})
feature_scores = feature_scores.sort_values(by='Mutual Info', ascending=False)

# Select top 10 features based on mutual information
selected_features = feature_scores['Feature'].head(10).tolist()
print("Selected Features:", selected_features)

# Visualize feature importance based on mutual information scores
plt.figure(figsize=(12, 8))
plt.barh(feature_scores['Feature'], feature_scores['Mutual Info'], color='skyblue')
plt.xlabel('Mutual Information Score')
plt.ylabel('Features')
plt.title('Features by Mutual Information Score')
plt.tight_layout()
plt.show()

#Outlier Detection (boxplot visualization)
plt.figure(figsize=(12, 6))
for i, feature in enumerate(selected_features):
    plt.subplot(2, 5, i + 1)  # Adjusted to fit 10 features
    sns.boxplot(x=x_train[feature])
    plt.title(feature)
plt.tight_layout()
plt.show()

# Step 3: Outlier Removal using IQR method
x_train_no_outliers = x_train[selected_features].copy()

# Remove outliers and create a mask for y_train
for feature in selected_features:
    Q1 = x_train_no_outliers[feature].quantile(0.25)
    Q3 = x_train_no_outliers[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Create a mask to filter out outliers
    mask = (x_train_no_outliers[feature] >= lower_bound) & (x_train_no_outliers[feature] <= upper_bound)
    x_train_no_outliers = x_train_no_outliers[mask]

# Get remaining indices after filtering
remaining_indices = x_train_no_outliers.index
y_train_no_outliers = y_train.loc[remaining_indices]  # Use the remaining indices to filter y_train

# Visualize the boxplots after outlier removal
plt.figure(figsize=(12, 6))
for i, feature in enumerate(selected_features):
    plt.subplot(2, 5, i + 1)  # Adjusted to fit 10 features
    sns.boxplot(x=x_train_no_outliers[feature])
    plt.title(f"{feature} (After Outlier Removal)")

plt.tight_layout()
plt.show()

# Step 4: Train Gradient Boosting Classifier
gbm_model = GradientBoostingClassifier()
gbm_model.fit(x_train_no_outliers, y_train_no_outliers)

# Step 5: Make predictions
y_pred = gbm_model.predict(x_test[selected_features])

# Step 6: Check accuracy and classification report
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print(classification_report(y_test, y_pred, zero_division=1))

# Step 7: Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

#step 8
# Import necessary libraries for widgets
import ipywidgets as widgets
from IPython.display import display

# Initialize input widgets for each selected feature
input_widgets = {feature: widgets.FloatText(description=feature) for feature in selected_features}

# Display the widgets
for widget in input_widgets.values():
    display(widget)

# Create a button for prediction
predict_button = widgets.Button(description="Predict")

# Define the prediction function
def make_prediction(button):
    input_data = pd.DataFrame([{feature: input_widgets[feature].value for feature in selected_features}])
    prediction = gbm_model.predict(input_data)

    # Provide prediction outcome
    if prediction[0] == 0:
        print("The prediction indicates a negative thyroid condition.")
    elif prediction[0] == 1:
        print("The prediction indicates compensated hyperthyroidism.")
    elif prediction[0] == 2:
        print("The prediction indicates compensated hypothyroidism.")
    else:
        print("The prediction indicates goiter.")

# Bind the prediction function to the button click event
predict_button.on_click(make_prediction)

# Display the button
display(predict_button)

from sklearn.model_selection import GridSearchCV

# Define a parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.7, 0.8, 1.0],
    'min_samples_split': [2, 3, 4]
}

# Initialize the GridSearchCV object with GradientBoostingClassifier and parameter grid
grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)

# Fit GridSearchCV on training data
grid_search.fit(x_train_no_outliers, y_train_no_outliers)